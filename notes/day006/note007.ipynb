{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"note007.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"i_v4XgFQM6u4","colab_type":"text"},"source":["ppt 10쪽 :   \n","weight를 gradient 반대 값으로 이동 -alpha * gard\n","alpha == learning rate"]},{"cell_type":"markdown","metadata":{"id":"-KKFEPjKIC6D","colab_type":"text"},"source":["11쪽 :  \n","weight를 바꿀 때 error가 얼마나 바뀌나   \n","y를 바꿀 때 f가 어떻게 바뀌나 ...  \n","-> 순서대로 거꾸로 감  "]},{"cell_type":"markdown","metadata":{"id":"EbUtpyPrWxEH","colab_type":"text"},"source":["hidden layer에서는 relu 사용  \n","output layer에서는 sigmoid, softmax 사용  \n","pixel-wise softmax : 픽셀마다 클래스로 분류  "]},{"cell_type":"markdown","metadata":{"id":"41pqaxGifUIB","colab_type":"text"},"source":["# [Tensorflow low level api](https://www.tensorflow.org/guide/low_level_intro)"]},{"cell_type":"code","metadata":{"id":"yzJUd5PvW_Wk","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jy3rnLzAaC_Q","colab_type":"code","colab":{}},"source":["class Neuron(object):\n","  def __init__(self, w_, b_):\n","    # _ : member변수임을 나타내는듯\n","    \n","    # 학습 대상은 Variable로 해야 함\n","    self.w_ = tf.Variable(w_, name='weight')  # 클래스의 맴버변수 self로...\n","    self.b_ = tf.Variable(b_, name='bias')\n","    self.output_ = tf.constant(0.0)\n","    \n","    # placeholder 우리가 넣어줘야 하는 값\n","    self.input_ = tf.placeholder(tf.float32, shape=[1], name=\"input_\")\n","    self.target_ = tf.placeholder(tf.float32, shape=[1], name=\"output_\")\n","  \n","  def get_activation(self, x):\n","    return x\n","  \n","  # y=x 함수라 편미분은 1임\n","  def get_activation_gradient(self, sigma):\n","    return 1.0\n","  \n","  def prop_backward(self):\n","    lr = 0.1\n","    \n","    grad = (self.output_ - self.target_) * 1.0 \\\n","    * self.get_activation_gradient(self.output_)\n","    self.w_ = self.w_ - (lr * grad * self.input_)\n","    self.b_ = self.b_ - (lr * grad * 1.0)\n","    \n","    return self.feed_forward()\n","  \n","  def feed_forward(self):\n","    sigma = self.w_ * self.input_ + self.b_\n","    self.output_ = self.get_activation(sigma)\n","    return self.output_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ciqxMsYcTF6","colab_type":"code","colab":{}},"source":["my_neuron = Neuron(2.0, 1.0)\n","\n","x = [1.0]\n","y = [4.0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6A4SymXfyiR-","colab_type":"code","outputId":"c584d547-c513-49ac-b83b-a805768646b8","executionInfo":{"status":"ok","timestamp":1562593702801,"user_tz":-540,"elapsed":749,"user":{"displayName":"김민우","photoUrl":"","userId":"05585706814939375266"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["type(my_neuron.w_)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensorflow.python.ops.variables.RefVariable"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"P4oJdABPx6Fa","colab_type":"code","colab":{}},"source":["with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  print(sess.run(my_neuron.feed_forward(), feed_dict={my_neuron.input_: x, my_neuron.target_ : y}))\n","  print()\n","  for i in range(50):\n","    print(sess.run(my_neuron.prop_backward(),feed_dict={my_neuron.input_: x, my_neuron.target_: y}))\n","    print(sess.run([my_neuron.w_, my_neuron.b_]))\n","    # weight & bias 출력해보기\n","    # 근데 왜 feed_dict 해줘야하나?? == 이미 그래프를 형성해서 feed_dict 안주면 에러가 난다. (의존성이 생기기 때문이다.)\n","    # tf2에서 eager tensor있어서 출력될 가능성 있음\n","    print()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAfnO1FncjfI","colab_type":"code","outputId":"05d3b28d-41eb-4315-abba-60fc8e9338ca","executionInfo":{"status":"ok","timestamp":1562591250929,"user_tz":-540,"elapsed":120233,"user":{"displayName":"김민우","photoUrl":"","userId":"05585706814939375266"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  print(sess.run(my_neuron.feed_forward(), feed_dict={my_neuron.input_: x, my_neuron.target_ : y}))\n","  print()\n","  for i in range(50):\n","    print(sess.run(my_neuron.prop_backward(),feed_dict={my_neuron.input_: x, my_neuron.target_: y}))\n","    print(sess.run([my_neuron.w_, my_neuron.b_],feed_dict={my_neuron.input_: x, my_neuron.target_: y}))\n","    # weight & bias 출력해보기\n","    # 근데 왜 feed_dict 해줘야하나\n","    print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[3.9999995]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n","[3.9999995]\n","[array([2.4999995], dtype=float32), array([1.4999999], dtype=float32)]\n","\n"],"name":"stdout"}]}]}